{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "import copy\n",
    "import pyro\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "from causal_models.trainer import preprocess_batch\n",
    "from train_setup import setup_directories, setup_tensorboard, setup_logging\n",
    "from train_setup import setup_dataloaders\n",
    "# From datasets import get_attr_max_min\n",
    "from utils import EMA, seed_all\n",
    "from dscm.dscm import DSCM\n",
    "from hvae2 import HVAE2\n",
    "import torch.nn.functional as F\n",
    "from pgm.train_pgm import sup_epoch, eval_epoch\n",
    "from pgm.utils_pgm import check_nan, update_stats, calculate_loss, plot_cf\n",
    "from dscm.dscm import vae_preprocess\n",
    "from pgm.layers import TraceStorage_ELBO\n",
    "from pgm.chest_pgm_segmentor import FlowPGM_with_seg \n",
    "\n",
    "from unet import ResUnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hparams:\n",
    "    def update(self, dict):\n",
    "        for k, v in dict.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "# Load predictors\n",
    "predictor_path = '../pgm/checkpoints/Left-Lung_volume_Right-Lung_volume_Heart_volume/sup_aux_mimic_256_64_with_segmentation/checkpoint.pt'\n",
    "print(f'\\nLoading predictor checkpoint: {predictor_path}')\n",
    "predictor_checkpoint = torch.load(predictor_path)\n",
    "predictor_args = Hparams()\n",
    "predictor_args.update(predictor_checkpoint['hparams'])\n",
    "\n",
    "predictor_args.loss_norm = \"l2\"\n",
    "predictor_args.setup = \"sup_seg\"\n",
    "\n",
    "# Load deep VAE\n",
    "beta = 3\n",
    "vae_path = f\"../checkpoints/Left-Lung_volume_Right-Lung_volume_Heart_volume/mimic_crop_256_64_beta_{beta}_segmentations/checkpoint.pt\"\n",
    "\n",
    "print(f'\\nLoading VAE checkpoint: {vae_path}')\n",
    "vae_checkpoint = torch.load(vae_path)\n",
    "vae_args = Hparams()\n",
    "vae_args.batch_size = 10\n",
    "\n",
    "vae_args.update(vae_checkpoint['hparams'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlowPGM_with_seg(predictor_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = setup_dataloaders(vae_args, cache=False, shuffle_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_count = 0\n",
    "for batch in tqdm(dataloaders[\"valid\"]):\n",
    "    for_count+=1\n",
    "    \n",
    "    segs = model.predict_segmentations(**batch)\n",
    "    if for_count>1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCEDiceloss(input, target):\n",
    "    pred = input.view(-1).type(torch.DoubleTensor)\n",
    "    truth = target.view(-1).type(torch.DoubleTensor)\n",
    "    # BCE loss\n",
    "    bce_loss = torch.nn.BCELoss()(pred, truth)\n",
    "    # Dice Loss\n",
    "    dice_coef = (2.0 * (pred * truth).float().sum() + 1) / (\n",
    "        pred.float().sum() + truth.float().sum() + 1\n",
    "    )\n",
    "    return bce_loss + (1 - dice_coef)\n",
    "\n",
    "def segmentation_loss(pred_batch, target_batch):\n",
    "    \"\"\"Calculate the segmentation loss.\"\"\"\n",
    "    loss=0\n",
    "    for k in pred_batch.keys():\n",
    "        assert pred_batch[k].size()==target_batch[k].size(), f\"{k} size does not match, pred_batch size {pred_batch[k].size()}; target batch size {target_batch[k].size()}\"\n",
    "        if k in [\"Left-Lung\", \"Right-Lung\", \"Heart\"]:\n",
    "            loss+=BCEDiceloss(pred_batch[k], target_batch[k])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = segmentation_loss(pred_batch=segs, target_batch=batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tian_breast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
