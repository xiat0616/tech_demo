{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "import copy\n",
    "import pyro\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "from train_setup import setup_directories, setup_tensorboard, setup_logging\n",
    "from train_setup import setup_dataloaders\n",
    "# From datasets import get_attr_max_min\n",
    "from utils import EMA, seed_all\n",
    "from dscm import DSCM\n",
    "from hvae2 import HVAE2\n",
    "import torch.nn.functional as F\n",
    "from pgm.train_pgm import sup_epoch, eval_epoch\n",
    "from pgm.utils_pgm import check_nan, update_stats, calculate_loss, plot_cf\n",
    "from dscm import vae_preprocess\n",
    "from pgm.layers import TraceStorage_ELBO\n",
    "from pgm.chest_pgm import FlowPGM\n",
    "\n",
    "from train_cf import norm, loginfo, preprocess, inv_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hparams:\n",
    "    def update(self, dict):\n",
    "        for k, v in dict.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "# Load predictors\n",
    "predictor_path = '../pgm/checkpoints/scanner_sex_finding/sup_aux_padchest/checkpoint.pt'\n",
    "print(f'\\nLoading predictor checkpoint: {predictor_path}')\n",
    "predictor_checkpoint = torch.load(predictor_path)\n",
    "predictor_args = Hparams()\n",
    "predictor_args.update(predictor_checkpoint['hparams'])\n",
    "\n",
    "predictor_args.loss_norm = \"l2\"\n",
    "\n",
    "predictor = FlowPGM(predictor_args).cuda()\n",
    "predictor.load_state_dict(predictor_checkpoint['ema_model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_path = '../pgm/checkpoints/scanner_sex_finding/sup_pgm_padchest/checkpoint.pt'\n",
    "print(f'\\nLoading PGM checkpoint: {pgm_path}')\n",
    "pgm_checkpoint = torch.load(pgm_path)\n",
    "pgm_args = Hparams()\n",
    "pgm_args.update(pgm_checkpoint['hparams'])\n",
    "pgm = FlowPGM(pgm_args).cuda()\n",
    "pgm.load_state_dict(pgm_checkpoint['ema_model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load HVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load deep VAE\n",
    "beta = 3\n",
    "vae_path = f\"../checkpoints/scanner_sex_finding/padchest224_224_beta_{beta}/checkpoint.pt\"\n",
    "\n",
    "print(f'\\nLoading VAE checkpoint: {vae_path}')\n",
    "vae_checkpoint = torch.load(vae_path)\n",
    "vae_args = Hparams()\n",
    "vae_args.batch_size = 10\n",
    "\n",
    "vae_args.update(vae_checkpoint['hparams'])\n",
    "\n",
    "vae = HVAE2(vae_args).cuda()\n",
    "vae.load_state_dict(vae_checkpoint['ema_model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = setup_dataloaders(vae_args, cache=False, shuffle_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_fn = TraceStorage_ELBO(num_particles=1)\n",
    "# test_stats = eval_epoch(predictor, dataloaders['valid'])\n",
    "# print('test | '+' - '.join(f'{k}: {v:.4f}' for k, v in test_stats.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DSCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hparams:\n",
    "    def update(self, dict):\n",
    "        for k, v in dict.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "args = Hparams()\n",
    "\n",
    "dscm_dir = \"padchest_beta_9_5_focus_finding_soft_lr_1e4_lagrange_lr_1_damping_10\"\n",
    "\n",
    "\n",
    "which_checkpoint=\"6500_checkpoint\"\n",
    "\n",
    "args.load_path = f'checkpoints/scanner_sex_finding/{dscm_dir}/{which_checkpoint}.pt'\n",
    "print(args.load_path)\n",
    "dscm_checkpoint = torch.load(args.load_path )\n",
    "args.update(dscm_checkpoint['hparams'])\n",
    "model = DSCM(args, pgm, predictor, vae)\n",
    "args.cf_particles =1\n",
    "model.load_state_dict(dscm_checkpoint['ema_model_state_dict'])\n",
    "model.cuda()\n",
    "\n",
    "# Set model require_grad to False\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_VMIN,_VMAX = -120, 120\n",
    "def undo_norm(pa):\n",
    "    for k, v in pa.items():\n",
    "        if k ==\"age\":\n",
    "            pa[k] = (v + 1) / 2 *100 # [-1,1] -> [0,100]\n",
    "    return pa\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "class MidpointNormalize(colors.Normalize):\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        v_ext = np.max( [ np.abs(self.vmin), np.abs(self.vmax) ] )\n",
    "        x, y = [-v_ext, self.midpoint, v_ext], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot(x, fig=None, ax=None, nrows=1, cmap='Greys_r', norm=None, cbar=False, set_cbar_ticks=True, logger=None):\n",
    "    m, n = nrows, x.shape[0] // nrows\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(m, n, figsize=(n * 4, 8))\n",
    "    im = []\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            idx = (i, j) if m > 1 else j\n",
    "            ax = [ax] if n == 1 else ax\n",
    "            _x = x[i * n + j].squeeze()\n",
    "            if norm is not None:\n",
    "                norm = MidpointNormalize(vmin=_x.min(), midpoint=0, vmax=_x.max())\n",
    "                # norm = colors.TwoSlopeNorm(vmin=_x.min(), vcenter=0., vmax=_x.max())\n",
    "            # logger.info(f\"ax[idx] is: {type(ax[idx])}, m: {m}, n: {n}, shape: {np.shape(ax[idx])}\")\n",
    "            _im = ax[idx].imshow(_x, cmap=cmap, norm=norm)\n",
    "            im.append(_im)\n",
    "            ax[idx].axes.xaxis.set_ticks([])\n",
    "            ax[idx].axes.yaxis.set_ticks([])\n",
    "\n",
    "    if cbar:\n",
    "        if fig:\n",
    "            fig.subplots_adjust(wspace=-0.275, hspace=0.25)\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                idx = [i, j] if m > 1 else j\n",
    "                # cbar_ax = fig.add_axes([\n",
    "                #     ax[idx].get_position().x0 + 0.0025, # left\n",
    "                #     ax[idx].get_position().y1, # bottom\n",
    "                #     0.003, # width\n",
    "                #     ax[idx].get_position().height # height\n",
    "                # ])\n",
    "                cbar_ax = fig.add_axes([\n",
    "                    ax[idx].get_position().x0,\n",
    "                    ax[idx].get_position().y0 - 0.015,\n",
    "                    ax[idx].get_position().width,\n",
    "                    0.0075\n",
    "                ])\n",
    "                cbar = plt.colorbar(im[i * n + j], cax=cbar_ax,\n",
    "                                    orientation=\"horizontal\")  # , ticks=mticker.MultipleLocator(25)) #, ticks=mticker.AutoLocator())\n",
    "                # cbar.ax.tick_params(rotation=0)\n",
    "                # cbar.ax.locator_params(nbins=5)\n",
    "                _x = x[i * n + j].squeeze()\n",
    "\n",
    "                if set_cbar_ticks:\n",
    "                    d = 20\n",
    "                    _vmin, _vmax = _x.min().abs().item(), _x.max().item()\n",
    "                    _vmin = -(_vmin - (_vmin % d))\n",
    "                    _vmax = _vmax - (_vmax % d)\n",
    "                    lt = [_vmin, 0, _vmax]\n",
    "\n",
    "                    if (np.abs(_vmin) - 0) > d or (_vmax - 0) > d:\n",
    "                        lt.insert(1, _vmin // 2)\n",
    "                        lt.insert(-2, _vmax // 2)\n",
    "                    cbar.set_ticks(lt)\n",
    "                else:\n",
    "                    cbar.ax.locator_params(nbins=5)\n",
    "                    cbar.formatter.set_powerlimits((0, 0))\n",
    "\n",
    "                cbar.outline.set_visible(False)\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_plot(save_path, obs, cfs, do, var_cf_x=None, num_images=10):\n",
    "    _ = plot_cf(obs['x'], cfs['x'], \n",
    "        inv_preprocess({k: v for k, v in obs.items() if k != 'x'}),  # pa\n",
    "        inv_preprocess({k: v for k, v in cfs.items() if k != 'x'}),  # cf_pa\n",
    "        inv_preprocess(do), # Counterfactual variance per pixel\n",
    "        var_cf_x = var_cf_x,\n",
    "        num_images=num_images,\n",
    "    )\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_cf(x, cf_x, pa, cf_pa, do, var_cf_x=None, num_images=8, logger=None):\n",
    "    n = num_images  # 8 columns\n",
    "    x = (x[:n].detach().cpu() + 1) * 127.5\n",
    "    cf_x = (cf_x[:n].detach().cpu() + 1) * 127.5\n",
    "    # logger.info(f\"x: {x.size()}\")\n",
    "    fs = 12  # font size\n",
    "    m = 3 if var_cf_x is None else 4  # nrows\n",
    "    s = 5\n",
    "    fig, ax = plt.subplots(m, n, figsize=(n * s, m * s))\n",
    "    # fig, ax = plt.subplots(m, n, figsize=(n*s, m*s+2))\n",
    "    # logger.info(f\"ax: {np.shape(ax)}\")\n",
    "    # logger.info(f\"ax[0]: {type(ax[0])} {np.shape(ax[0])}, m: {m}, s: {s}, n: {n}\")\n",
    "    _, _ = plot(x, ax=ax[0])\n",
    "    _, _ = plot(cf_x, ax=ax[1])\n",
    "    _, _ = plot(cf_x - x, ax=ax[2], fig=fig, cmap='RdBu_r', cbar=True,\n",
    "                norm=MidpointNormalize(midpoint=0))\n",
    "    if var_cf_x is not None:\n",
    "        _, _ = plot(var_cf_x[:n].detach().sqrt().cpu(),\n",
    "                    fig=fig, cmap='jet', ax=ax[3], cbar=True, set_cbar_ticks=False)\n",
    "\n",
    "    sex_categories = ['male', 'female']  # 0, 1\n",
    "    finding_categories = ['No finding', 'Finding']  # 0, 1\n",
    "    scanner_categories = ['Phillips', 'Imaging']  # 0, 1\n",
    "\n",
    "    for j in range(n):\n",
    "        msg = ''\n",
    "        for i, (k, v) in enumerate(do.items()):\n",
    "            if k == 'sex':\n",
    "                vv = sex_categories[int(v[j].item())]\n",
    "                kk = 's'\n",
    "            elif k == 'finding':\n",
    "                vv = finding_categories[int(v[j].item())]\n",
    "                kk = 'f'\n",
    "            elif k == 'scanner':\n",
    "                vv = scanner_categories[int(v[j].item())]\n",
    "                kk = 'sc'\n",
    "            else:\n",
    "                continue\n",
    "            msg += kk + '{{=}}' + vv\n",
    "            msg += ', ' if (i + 1) < len(list(do.keys())) else ''\n",
    "\n",
    "        if 'sex' in pa.keys():\n",
    "            s = str(sex_categories[int(pa['sex'][j].item())])\n",
    "            f = str(finding_categories[int(pa['finding'][j].item())])\n",
    "            sc = str(scanner_categories[int(pa['scanner'][j].item())])\n",
    "\n",
    "        if 'sex' in pa.keys():\n",
    "            ax[0, j].set_title(f's={s}, f={f}, sc={sc}',\n",
    "                               pad=8, fontsize=fs, \n",
    "                               multialignment='center', linespacing=1.5)\n",
    "            ax[1, j].set_title(f'do(${msg}$)', fontsize=fs, pad=10)\n",
    "\n",
    "        # Plot counterfactual\n",
    "        if 'sex' in cf_pa.keys():\n",
    "            cf_s = str(sex_categories[int(cf_pa['sex'][j].item())])\n",
    "            cf_f = str(finding_categories[int(cf_pa['finding'][j].item())])\n",
    "            cf_sc = str(scanner_categories[int(cf_pa['scanner'][j].item())])\n",
    "\n",
    "            ax[1, j].set_xlabel(\n",
    "                rf'$\\widetilde{{s}}{{=}}{cf_s}, \\ \\widetilde{{f}}{{=}}{cf_f}, \\ \\widetilde{{sc}}{{=}}{cf_sc}$',\n",
    "                labelpad=9, fontsize=fs, multialignment='center', linespacing=1.25)\n",
    "\n",
    "    ax[0, 0].set_ylabel('Observation', fontsize=fs + 2, labelpad=8)\n",
    "    ax[1, 0].set_ylabel('Counterfactual', fontsize=fs + 2, labelpad=8)\n",
    "    ax[2, 0].set_ylabel('Treatment Effect', fontsize=fs + 2, labelpad=8)\n",
    "    if var_cf_x is not None:\n",
    "        ax[3, 0].set_ylabel('Uncertainty', fontsize=fs + 2, labelpad=8)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Hparams()\n",
    "args.save_dir = f\"results_tech_demo/{dscm_dir}/{which_checkpoint}\"\n",
    "os.makedirs(args.save_dir , exist_ok=True)\n",
    "\n",
    "model.pgm.eval()\n",
    "model.predictor.eval()\n",
    "model.vae.eval()\n",
    "dag_variables = list(model.pgm.variables.keys())\n",
    "preds = {k: [] for k in dag_variables}\n",
    "targets = {k: [] for k in dag_variables}\n",
    "# args.save_dir = f\"../../results/{dscm_dir}/{which_checkpoint}\"\n",
    "# os.makedirs(args.save_dir , exist_ok=True)\n",
    "# loader = tqdm(enumerate(dataloaders['test']), total=len(\n",
    "#     dataloaders['test']), mininterval=0.1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_counterfactuals(model, dataloader, predictor, do_pa=None):\n",
    "    ' this can consume lots of memory if dataset is large'\n",
    "    model.pgm.eval()\n",
    "    model.predictor.eval()\n",
    "    predictor.eval()\n",
    "    model.vae.eval()\n",
    "    dag_variables = list(model.pgm.variables.keys())\n",
    "    preds = {k: [] for k in dag_variables}\n",
    "    targets = {k: [] for k in dag_variables}\n",
    "    plt_counter = 0\n",
    "    cf_particles=1\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        # if plt_counter>10:\n",
    "        #     continue\n",
    "        plt_counter+=1\n",
    "        bs = batch['x'].shape[0]\n",
    "        batch = preprocess(batch)\n",
    "        # randomly intervene on a single parent, where pa_k ~ p(pa_k)\n",
    "        do = {}\n",
    "        do_k = copy.deepcopy(do_pa) if do_pa else random.choice(dag_variables) \n",
    "\n",
    "        # do[do_k] = batch[do_k].clone()[torch.randperm(bs)]\n",
    "\n",
    "        do[do_k] =1-  batch[do_k].clone()\n",
    "        do = preprocess(norm(do))\n",
    "\n",
    "        # get counterfactual pa\n",
    "        pa = {k: v for k, v in batch.items() if k != 'x'}     \n",
    "        _pa = vae_preprocess(\n",
    "            vae_args, {k: v.clone() for k, v in pa.items()})   \n",
    "        # cf_pa = model.pgm.counterfactual(obs=pa, intervention=do, num_particles=1)       \n",
    "        \n",
    "        # get counterfactual x\n",
    "        out = model.forward(batch, do, elbo_fn, cf_particles=cf_particles)\n",
    "        cf_pa = out['cf_pa']\n",
    "        _cf_pa = vae_preprocess(\n",
    "                vae_args, {k: v.clone() for k, v in cf_pa.items()})\n",
    "\n",
    "        nans = 0\n",
    "        for k, v in out['cfs'].items():\n",
    "        # for k, v in cfs.items():\n",
    "            k_nans = torch.isnan(v).sum()\n",
    "            nans += k_nans\n",
    "            if k_nans > 0:\n",
    "                print(f'\\nFound {k_nans} nans in cf {k}.')\n",
    "        if nans > 0:\n",
    "            continue\n",
    "\n",
    "        predict_out = predictor.predict(**out['cfs'])\n",
    "        # predict_out = model.predictor.predict(**cfs)\n",
    "\n",
    "\n",
    "        for k, v in predict_out.items():\n",
    "            preds[k].extend(v)\n",
    "        \n",
    "        # interventions are the targets for prediction\n",
    "        for k in targets.keys():\n",
    "            if k in do.keys():\n",
    "                targets[k].extend(\n",
    "                    do[k]\n",
    "                )\n",
    "            else:\n",
    "                targets[k].extend(\n",
    "                    cf_pa[k]\n",
    "                )\n",
    "\n",
    "        if plt_counter<20:\n",
    "            save_path = os.path.join(args.save_dir, f'test_{do_k}_{plt_counter}_cfs.png')\n",
    "            save_plot(save_path, batch, out['cfs'], do,  \n",
    "                      var_cf_x = out['var_cf_x'],\n",
    "                      num_images=4)\n",
    "        # else:\n",
    "        #     break\n",
    "     \n",
    "    for k, v in preds.items():\n",
    "        preds[k] = torch.stack(v).squeeze().cpu()\n",
    "        targets[k] = torch.stack(targets[k]).squeeze().cpu()\n",
    "        # print(f'{k} | preds: {preds[k].shape} - targets: {targets[k].shape}')\n",
    "\n",
    "    stats = {}\n",
    "    for k in dag_variables:\n",
    "        if k in ['sex', 'finding', 'scanner']:\n",
    "            stats[k+'_rocauc'] = roc_auc_score(\n",
    "                targets[k].numpy(), preds[k].numpy(), average='macro')\n",
    "            # stats[k+'_acc'] = (targets[k].squeeze(-1) == torch.round(preds[k])).sum().item() / targets[k].shape[0]\n",
    "        elif k == 'age':\n",
    "            stats[k] = torch.mean(torch.abs(targets[k] - preds[k])).item() * 100\n",
    "        elif k == 'race':\n",
    "            num_corrects = (targets[k].argmax(-1) == preds[k].argmax(-1)).sum()\n",
    "            stats[k + \"_acc\"] = num_corrects.item() / targets[k].shape[0]\n",
    "            stats[k + \"_rocauc\"] = roc_auc_score(\n",
    "                targets[k].numpy(),\n",
    "                preds[k].numpy(),\n",
    "                multi_class=\"ovr\",\n",
    "                average=\"macro\",)\n",
    "    return stats, preds, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_do = {\n",
    "    'scanner':{}, \n",
    "    'sex':{}, \n",
    "    'finding':{},\n",
    "            }\n",
    "preds_do = {\n",
    "    'scanner':{}, \n",
    "    'sex':{}, \n",
    "    'finding':{},\n",
    "            }\n",
    "targets_do = {\n",
    "    'scanner':{}, \n",
    "    'sex':{}, \n",
    "    'finding':{},\n",
    "            }\n",
    "\n",
    "# base_stats = {\n",
    "#     'race_rocauc':0.8468415443534377,\n",
    "#     'sex_rocauc': 0.996207396871533,\n",
    "#     'finding_rocauc':0.9443025803264049,\n",
    "#     'age':6.17903545498848 ,\n",
    "# }\n",
    "\n",
    "for do_v in stats_do.keys():\n",
    "    stats, preds, targets = eval_counterfactuals(model, dataloaders['valid'], predictor, do_pa=do_v)\n",
    "    # stats = eval_random(model, dataloaders['valid'], do_pa=do_v)\n",
    "    stats_do[do_v] = stats\n",
    "\n",
    "# stats, preds, targets = eval_counterfactuals(model, dataloaders['valid'], predictor_for_evaluation)\n",
    "# # stats = eval_random(model, dataloaders['valid'])\n",
    "# stats_do['random'] = stats\n",
    "\n",
    "for do_v in stats_do.keys():\n",
    "    print(f'do_{do_v} | '+' - '.join(f'{k}: {v:.3f}' for k,v in stats_do[do_v].items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for do_v in stats_do.keys():\n",
    "    print(f'do_{do_v} | '+' - '.join(f'{k}: {v:.3f}' for k,v in stats_do[do_v].items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tian_breast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
